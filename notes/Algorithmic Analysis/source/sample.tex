\documentclass[12pt,a4paper,olive]{bbe}
\usepackage{blindtext}
\begin{document}
			
	\chapter{Algorithmic Analysis}
	\section{Correctness}
	A problem X is defined as an ordered (q,a) set where 'q' is a question and 'a' is an answer, written mathematically as: 
 
	
	$$X={((p(x),r(x)) | x \exists D)}$$
	With a 'D' set of data.

	An algorithm is considered correct if it solves a problem given A input, resulting in a desired B output in all possible cases.
	\section{Complexity}
	Algorithmic complexity is the approximated cost of an algorithm when running on a machine capable of computing. This kind of analysis can be quantified in time, memory, and even operational costs.

	\begin{remark}
	The method to define and value algorithms must be independent of programming language, architecture, and even more broadly, of the specific machine the algorithm runs on. This implies we don't take into account computational problems, such as compilation or, in interpreted languages, the interpretation into bytecode of the algorithm, for this doesn't concern the algorithm, but rather factors external to it.   
	\end{remark}

	\subsection{R-Complexity}
	R is the class of decision problems solvable by a Turing machine. The main idea of it is defining a sum of the costs inside of the machine according to the operations realized by the algorithm. Resulting mathematically in the total cost of the algorithm's execution. However, R-Complexity is generally hard to compare between programming languages and individual machines because of the dependance of the result in constants.

	given this definition, take as an example:

	\begin{verbatim}
	#Python pseudocode, take a list and copy it into another.
	def Solution(list nums):
		answer = []
		for i in list:	
			answer.append(i)
		return answer
	\end{verbatim}

	\subsection{Complexity Order}
	be it 'f' and 'q' functions such as: 

	\subsubsection{Order Theorems}
	\begin{itemize}
		\item Constant rule: $$(\forall c  \exists  N | : cf = O(f))$$
		\item {
			Sum rule: $$ f + g = O(max(f,g)) $$
			\subparagraph*{proven as:}
			
			}
		\item {
			Product rule: $$ f = O(r) \land g = O(s) \longrightarrow fg = O(rs) $$
			\subparagraph*{proven as:}
			
			}
	\end{itemize}

	\subsubsection{Order classification examples}
	\begin{enumerate}
		\item $$2n^5 = O(n^5)$$
		\item $$2n^5 + 4n = O(n^5) $$
		\item $$ (2n^5 + 4n)^2 = O(n^{10})$$
		\item $$(n+1)^2 = O(n^2)$$
	\end{enumerate}

	\subsection{Complexity orders}
	Complexity orders are classified as follows, according to big O notation.
	\begin{itemize}
		\item {
			\subparagraph*{$$O(1)$$} 
		Constant: Execution time doesn't depend on the size of the input
		} 
		\item {
			\subparagraph{$$O(log(n))$$}
		Logarithmic: Execution time grows slower than the input size, Ex: binary search
		}
		\item {
			\subparagraph{$$O(n)$$}
		Linear: Execution time grows at the same rate as the input size, Ex. Unordered array search 
		}
		\item {
			\subparagraph{$$O(n log(n))$$}
		Log-Linear: Execution time grows at the same rate as the input size but does O(log(n)) operations per item
		}
		\item {
			\subparagraph{$$O(n^2)$$}
		Quadratic: Execution time grows at a linear rate related to the input size, but requires a linear operation per item in the input.
		}
		\item {
			\subparagraph{$$O(2^n)$$}
		Exponential: Execution time grows exponentially compared to the input
		}
	\end{itemize}

	\


	
 
\end{document}